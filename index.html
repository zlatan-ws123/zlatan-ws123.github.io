<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æŠ€æœ¯æŠ¥å‘Š | Technical Report</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Font Awesome -->
    <link href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <!-- Josefin Sans å­—ä½“ -->
    <link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap" rel="stylesheet">
    <!-- é˜¿é‡Œå¦ˆå¦ˆå­—ä½“ -->
    <link rel="stylesheet" href="https://at.alicdn.com/t/c/font_3774118_98h3w3c348.css">
	
	<script>
        // é…ç½®Tailwindè‡ªå®šä¹‰ä¸»é¢˜
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#1a1a1a',
                        secondary: '#666666',
                        accent: '#ff3e00',
                        light: '#f5f5f5',
                        dark: '#121212'
                    },
                    fontFamily: {
                        'josefin': ['"Josefin Sans"', 'sans-serif'],
                        'alimama': ['"Alimama FangYuan"', 'sans-serif']
                    },
                }
            }
        }
    </script>
	
	<script>
	// é…ç½®MathJax - ä¿æŒåŸæœ‰æ ·å¼
        window.MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)']],
                displayMath: [['\\[', '\\]']]
            },
            options: {
                // é˜²æ­¢MathJaxæ ·å¼è¢«è¦†ç›–
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                renderActions: {
                    // ä¿æŠ¤MathJaxçš„å­—ä½“è®¾ç½®
                    addMenu: [0, '', '']
                }
            }
        };
	</script>
	
	<style type="text/tailwindcss">
        @layer utilities {
            .text-balance {
                text-wrap: balance;
            }
            .content-visibility-auto {
                content-visibility: auto;
            }
            .section-fade-in {
                opacity: 0;
                transform: translateY(20px);
                transition: opacity 0.6s ease-out, transform 0.6s ease-out;
            }
            .section-visible {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        /* å…¨å±€æ ·å¼ */
        body {
            font-feature-settings: "palt";
            letter-spacing: 0.02em;
        }
        
        /* è‹±æ–‡ä½¿ç”¨Josefin Sans */
        :lang(en) {
            font-family: 'Josefin Sans', sans-serif !important;
        }
        
        /* ä¸­æ–‡ä½¿ç”¨é˜¿é‡Œå¦ˆå¦ˆå­—ä½“ */
        :lang(zh) {
            font-family: "Alimama FangYuan", sans-serif !important;
        }
        
        /* å¹³æ»‘æ»šåŠ¨ */
        html {
            scroll-behavior: smooth;
        }
        
        /* å›¾ç‰‡æ‚¬åœæ•ˆæœ */
        .img-hover-zoom {
            overflow: hidden;
        }
        .img-hover-zoom img {
            transition: transform 0.5s ease;
        }
        .img-hover-zoom:hover img {
            transform: scale(1.03);
        }
		.MathJax, .MathJax * {
			font-family: "mathjax-tex" !important; /* é‡ç½®å­—ä½“å®¶æ— */
			font-size: initial !important;   /* é‡ç½®å­—ä½“å¤§å° */
			color: initial !important;       /* é‡ç½®å­—ä½“é¢œè‰² */
			/* æ·»åŠ å…¶ä»–ä½ éœ€è¦é‡ç½®çš„å±æ€§ */
		}
		.math-container {
			font-family: 'mathjax-tex' !important;
			font-size: 20px;
		}
    
        
	</style>
	
	<!-- å¼•å…¥Prism.jsæ ·å¼å’Œè„šæœ¬ -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
	<!-- æŒ‰éœ€å¼•å…¥è¯­è¨€æ”¯æŒ -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
	<!-- å¼•å…¥MathJax -->
	<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	
	
    
</head>

<body class="bg-light text-primary antialiased">
    <!-- å¯¼èˆªæ  -->
    <header class="fixed top-0 left-0 right-0 z-50 transition-all duration-300" id="navbar">
        <nav class="container mx-auto px-4 md:px-8 py-6 flex justify-between items-center">
            <div class="flex items-center space-x-1">
                <span class="text-2xl font-josefin font-bold tracking-wider" lang="en" style="color:gray;">Lucky</span>
				<span class="text-2xl font-josefin font-bold tracking-wider" lang="en">Model</span>
                <span class="h-4 w-px bg-primary/30 mx-2"></span>
                <span class="text-lg font-alimama" lang="zh">TECH REPORT</span>
            </div>
            
            <!-- æ¡Œé¢å¯¼èˆª -->
            <ul class="hidden md:flex space-x-8 items-center">
                <li><a href="#intro" class="font-josefin hover:text-accent transition-colors" lang="en">Introduction</a></li>
                <li><a href="#methodology" class="font-josefin hover:text-accent transition-colors" lang="en">Methodology</a></li>
                <li><a href="#results" class="font-josefin hover:text-accent transition-colors" lang="en">Results</a></li>
                <li><a href="#discussion" class="font-alimama hover:text-accent transition-colors" lang="en">Discussion</a></li>
                <li><a href="#conclusion" class="font-josefin hover:text-accent transition-colors" lang="en">Conclusion</a></li>
            </ul>
            
            <!-- ç§»åŠ¨ç«¯èœå•æŒ‰é’® -->
            <button class="md:hidden text-xl" id="menuToggle">
                <i class="fa fa-bars"></i>
            </button>
        </nav>
        
        <!-- ç§»åŠ¨ç«¯å¯¼èˆªèœå• -->
        <div class="md:hidden bg-white shadow-lg absolute w-full left-0 top-full transform -translate-y-full opacity-0 transition-all duration-300 pointer-events-none" id="mobileMenu">
            <ul class="container mx-auto px-4 py-4 flex flex-col space-y-4">
                <li><a href="#intro" class="block py-2 font-josefin" lang="en">Introduction</a></li>
                <li><a href="#methodology" class="block py-2 font-josefin" lang="en">Methodology</a></li>
                <li><a href="#results" class="block py-2 font-josefin" lang="en">Results</a></li>
                <li><a href="#discussion" class="block py-2 font-alimama" lang="en">Discussion</a></li>
                <li><a href="#conclusion" class="block py-2 font-josefin" lang="en">Conclusion</a></li>
            </ul>
        </div>
    </header>

    <!-- è‹±é›„åŒºåŸŸ -->
    <section class="pt-32 pb-20 md:pt-40 md:pb-32 px-4 md:px-8">
        <div class="container mx-auto max-w-6xl">
            <div class="grid grid-cols-1 md:grid-cols-12 gap-8 items-center">
                <div class="md:col-span-8">
					<!--
                    <div class="inline-block px-3 py-1 border border-primary/20 rounded-full mb-6">
                        <span class="text-sm font-josefin tracking-wider" lang="en">TECHNICAL REPORT</span>
                    </div>
					 -->
                    <h1 class="text-4xl md:text-6xl lg:text-7xl font-josefin font-bold leading-tight mb-6" lang="en">
                        Train a Unified <br>
                        <span class="text-accent">Multimodal Understanding and Generation Model </span> <br>
						from Scratch
                    </h1>
					<!--
                    <p class="text-xl md:text-2xl text-secondary max-w-xl mb-8 font-alimama leading-relaxed" lang="zh">
                        æœ¬æŠ¥å‘Šæ¢è®¨äº†æ–°ä¸€ä»£ç³»ç»Ÿæ¨¡å‹çš„è®¾è®¡åŸç†ä¸åº”ç”¨å‰æ™¯ï¼Œç»“åˆç†è®ºåˆ†æä¸å®éªŒæ•°æ®ï¼Œä¸ºæŠ€æœ¯å‘å±•æä¾›æ–¹å‘å‚è€ƒã€‚
                    </p>
					-->
                    <div class="flex flex-wrap items-center gap-4 text-sm text-secondary">
                        <div class="flex items-center">
                            <!--<i class="fa fa-calendar-o mr-2"></i> -->
                            <span class="font-josefin" lang="en">September 2025</span>
                        </div>
                        <div class="h-3 w-px bg-secondary/30"></div>
                        <div class="flex items-center">
                            <!--<i class="fa fa-clock-o mr-2"></i> -->
                            <span class="font-josefin" lang="en">15 min read</span>
                        </div>
                    </div>
                </div>
                <div class="md:col-span-4 img-hover-zoom">
                    <img src="../pic_7_0_img7.png" alt="æŠ½è±¡æ’å›¾" class="w-full h-auto object-cover rounded-lg shadow-xl">
					<p class="text-sm text-secondary font-josefin text-center" lang="en">
						A sampling in the early stages of model training &#128514;
					</p>
                </div>
            </div>
        </div>
    </section>

    <!-- ç›®å½• -->
	<!--
    <section class="py-12 bg-white">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <div class="border-l-4 border-accent pl-6 mb-10">
                <h2 class="text-2xl md:text-3xl font-josefin font-semibold" lang="en">Table of Contents</h2>
                <p class="text-secondary mt-2 font-alimama" lang="zh">æŠ¥å‘Šå†…å®¹æ¦‚è§ˆ</p>
            </div>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <div class="flex items-start group">
                    <span class="text-4xl font-josefin font-bold text-accent/20 group-hover:text-accent transition-colors mr-4">01</span>
                    <div>
                        <h3 class="text-xl font-josefin font-semibold mb-2 group-hover:text-accent transition-colors" lang="en">Introduction</h3>
                        <p class="text-secondary font-alimama" lang="zh">ç ”ç©¶èƒŒæ™¯ã€ç›®çš„ä¸æ„ä¹‰æ¦‚è¿°</p>
                    </div>
                </div>
                
                <div class="flex items-start group">
                    <span class="text-4xl font-josefin font-bold text-accent/20 group-hover:text-accent transition-colors mr-4">02</span>
                    <div>
                        <h3 class="text-xl font-josefin font-semibold mb-2 group-hover:text-accent transition-colors" lang="en">Methodology</h3>
                        <p class="text-secondary font-alimama" lang="zh">ç ”ç©¶æ–¹æ³•ä¸æ¨¡å‹æ„å»ºè¿‡ç¨‹</p>
                    </div>
                </div>
                
                <div class="flex items-start group">
                    <span class="text-4xl font-josefin font-bold text-accent/20 group-hover:text-accent transition-colors mr-4">03</span>
                    <div>
                        <h3 class="text-xl font-josefin font-semibold mb-2 group-hover:text-accent transition-colors" lang="en">Results</h3>
                        <p class="text-secondary font-alimama" lang="zh">å®éªŒç»“æœä¸æ•°æ®åˆ†æ</p>
                    </div>
                </div>
                
                <div class="flex items-start group">
                    <span class="text-4xl font-josefin font-bold text-accent/20 group-hover:text-accent transition-colors mr-4">04</span>
                    <div>
                        <h3 class="text-xl font-josefin font-semibold mb-2 group-hover:text-accent transition-colors" lang="zh">è®¨è®º</h3>
                        <p class="text-secondary font-alimama" lang="zh">ç»“æœè§£è¯»ä¸å…³é”®å‘ç°æ¢è®¨</p>
                    </div>
                </div>
                
                <div class="flex items-start group md:col-span-2">
                    <span class="text-4xl font-josefin font-bold text-accent/20 group-hover:text-accent transition-colors mr-4">05</span>
                    <div>
                        <h3 class="text-xl font-josefin font-semibold mb-2 group-hover:text-accent transition-colors" lang="en">Conclusion</h3>
                        <p class="text-secondary font-alimama" lang="zh">ç ”ç©¶æ€»ç»“ã€å±€é™ä¸æœªæ¥å±•æœ›</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
	 -->

    <!-- å¼•è¨€éƒ¨åˆ† -->
    <section id="intro" class="py-20 bg-white section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <div class="grid grid-cols-1 md:grid-cols-12 gap-12">
                <div class="md:col-span-12">
                    <span class="text-sm font-josefin uppercase tracking-widest text-accent" lang="en">Section 01</span>
                    <h2 class="text-3xl md:text-4xl font-josefin font-bold mt-2 mb-8" lang="en">Abstract</h2>
						<!--
                        <p class="text-lg leading-relaxed font-alimama" lang="zh">
                            åœ¨æ•°å­—åŒ–è½¬å‹åŠ é€Ÿæ¨è¿›çš„ä»Šå¤©ï¼Œç³»ç»Ÿæ¨¡å‹çš„è®¾è®¡ä¸ä¼˜åŒ–æˆä¸ºæŠ€æœ¯åˆ›æ–°çš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚æœ¬ç ”ç©¶èšç„¦äºæ–°ä¸€ä»£å¤æ‚ç³»ç»Ÿçš„å»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡ã€é«˜åŠ¨æ€æ€§æ•°æ®æ—¶é¢ä¸´çš„æ•ˆç‡ä¸ç²¾åº¦ç“¶é¢ˆã€‚
                        </p>
                        -->
						
                        <p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
                            This study proposes a unified multimodal understanding and generation framework. Our framework employs continuous representation of an image tokens for both understanding and generation, combining autoregressive language modeling with diffusion. It also decouples the visual encoding for multimodal understanding and generation while processing different modal data with the same Transformer architecture. We trained the model from scratch on a mixture of text and image data. This study specifically implements an autoregressive-diffusion hybrid architecture, with all experiments conducted on a single AMD 7900XTX GPU for training and validation.
                        </p>
						<br><br>
						
                        <!--
                        <blockquote class="border-l-4 border-accent pl-6 italic text-secondary my-8">
                            <p class="text-lg font-alimama" lang="zh">
                                "æ¨¡å‹çš„ä»·å€¼ä¸ä»…åœ¨äºå…¶å¯¹ç°å®çš„æŠ½è±¡èƒ½åŠ›ï¼Œæ›´åœ¨äºå…¶æŒ‡å¯¼å®è·µã€é¢„æµ‹æœªæ¥çš„å¯èƒ½æ€§ã€‚"
                            </p>
                        </blockquote>
						 -->
                        <!--
                        <p class="text-lg leading-relaxed font-alimama" lang="zh">
                            æœ¬æŠ¥å‘Šæ•´åˆäº†è·¨å­¦ç§‘ç ”ç©¶æˆæœï¼Œç»“åˆè®¡ç®—æœºç§‘å­¦ã€æ•°å­¦ä¸å·¥ç¨‹å­¦çš„ç†è®ºæ¡†æ¶ï¼Œæå‡ºäº†ä¸€å¥—å…¨æ–°çš„æ¨¡å‹è®¾è®¡æ–¹æ³•è®ºã€‚é€šè¿‡å®é™…æ¡ˆä¾‹éªŒè¯ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿçš„å“åº”é€Ÿåº¦ä¸å†³ç­–å‡†ç¡®æ€§ã€‚
                        </p>
						-->
                </div>
				<div class="md:col-span-5 p-6 bg-light rounded-lg border border-gray-100">
					<h3 class="text-lg font-josefin font-semibold mb-4" lang="en">Key Approaches</h3>
					<ul class="space-y-3 font-alimama" lang="en">
						<li class="flex items-start">
							<span>â€¢ A gating architecture based on cross-attention mechanism and direct concatenation</span>
						</li>
						<li class="flex items-start">
							<span>â€¢ Visual understanding and image generation adopt a decoupled architecture</span>
						</li>
						<li class="flex items-start">
							<span>â€¢ Multi-stage progressive training</span>
						</li>
						<li class="flex items-start">
							<span>â€¢ VAE-free end-to-end image generation architecture</span>
						</li>
					</ul>
				</div>
				<div class="md:col-span-7 top-32">
					<div class="rounded-lg overflow-hidden mb-6">
						<img src="arc0.png" alt="Overall architecture of the model" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin" lang="en">
						Overall architecture of the model
					</p>
				</div>
                
                <div class="md:col-span-4 flex flex-col justify-center">
                    
                </div>
            </div>
        </div>
    </section>

    <!-- æ–¹æ³•è®ºéƒ¨åˆ† -->
    <section id="methodology" class="py-20 section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
			<div class="grid grid-cols-1 md:grid-cols-12 gap-12">
				<div class="md:col-span-12">
					<span class="text-sm font-josefin uppercase tracking-widest text-accent" lang="en">Section 02</span>
					<h2 class="text-3xl md:text-4xl font-josefin font-bold mt-2 mb-8" lang="en">Methodology</h2>      
					<h3 class="text-2xl font-josefin font-semibold" lang="en">Model Architecture</h3>
                </div>
				<div class="md:col-span-7" id="textContent">
					<h4 class="text-2xl font-josefin font-light" lang="en">Multimodel Understanding</h4>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						Current multimodal large models typically employ pre-trained LLMs as their backbone network, where visual modality information is first tokenized through a visual encoder before being fused with text tokens. During training, these two modalities are aligned to achieve multimodal understanding capabilities. There are two predominant approaches for fusing visual and textual information:  Direct concatenation, Visual tokens and text tokens are simply concatenated and injected into the LLM for autoregressive generation; Cross-attention injection, Text information is injected into different layers of the language model backbone via cross-attention mechanisms.

					</p>
					
				</div>
				<div class="md:col-span-5" id="imageContainer">
					<div class="rounded-lg overflow-hidden">
						<img src="arc.png" alt="Architecture of the Multimodal Understanding model" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin" lang="en">
						Architecture of the Multimodal Understanding model
					</p>
				</div>
				<div class="md:col-span-12">
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						Previous research[@laurenconWhatMattersWhen2024] found that direct concatenation generally outperforms cross-attention injection when training language backbone models. However, while the multi-head self-attention mechanism computes attention scores with O(nÂ²) complexity, direct concatenation increases sequence length, leading to a significant surge in computational load. To reduce computational costs, some lightweight multimodal models still opt for cross-attention architectures. To combine the advantages of both approaches, this study proposes a multi-scale multimodal fusion method.
					</p>
					
				</div>
				<div class="md:col-span-12">
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						This study processes images through a visual encoder followed by further compression via downsampling. Similar to OmniVLM[@chenOmniVLMTokenCompressedSubBillionParameter2024], we employ a 1D convolutional layer (kernel size ğ‘˜ = 9, stride ğ‘  = 9, no padding) to minimize the number of tokens concatenated into the context. Additionally, intermediate layers from the visual encoder with higher fine-grained features are incorporated through cross-attention operations at each decoder layer. The output from the final cross-attention layer is gated and weighted before being summed with the attention layer output, with the combined result subsequently fed into the FFN layer:
					</p>
					<br>
					<!-- è¡Œå†…å…¬å¼ç¤ºä¾‹ -->
					<p>
					  <img src="result1.svg" alt="SVG Image" height="200" style="display: block; margin: 0 auto;">
					</p>
					<br>
					<p>
					  <img src="result2.svg" alt="SVG Image" height="200" style="display: block; margin: 0 auto;">
					</p>
				</div>
				<div class="md:col-span-12">
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>2D Rotary Position Embedding:</b> Early multimodal large models often lacked specialized positional encoding for visual data, simply flattening tokens from all modalities and applying standard 1D positional encoding. Subsequent research[@SuJianLinBiMenZaoCheZhiDuoMoTaiSiLuQianTanSanWeiZhiBianMa2024], [@baiQwen25VLTechnicalReport2025] extended RoPE-1D to two or even three dimensions - intuitively allocating half the positional features to the first dimension and the remaining half to the second dimension. This approach offers dual advantages: it more effectively models positional relationships between adjacent image patches while maintaining compatibility with the backbone network's pretrained positional encoding capabilities. Notably, RoPE-2D automatically reduces to RoPE-1D when the context contains only text tokens. Our study adopts this positional encoding scheme as well. Since the visual tokens in the self-attention layer are derived from downsampled visual encoder outputs, we replicate each token in the cross-attention layer by the downsampling factor to maintain positional index correspondence and achieve spatial alignment.					</p>
					</p>
				</div>
				<div class="md:col-span-5" id="imageContainer">
					<div class="rounded-lg overflow-hidden">
						<img src="2dpos.png" alt="RoPE-2D" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
						RoPE-2D
					</p>
				</div>
				<div class="md:col-span-2">
				</div>
				<div class="md:col-span-5" id="imageContainer">
					<div class="rounded-lg overflow-hidden">
						<img src="causalmask.png" alt="Causal mask" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
						Causal mask
					</p>
				</div>
				<div class="md:col-span-12">
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Causal mask:</b> We enable information interaction between different patches from the same image to achieve local bidirectional attention effects. To maintain compatibility with autoregressive tasks, text tokens still preserve unidirectional attention through causal masking. 					</p>
					</p>
				</div>
				<div class="md:col-span-12" id="textContent">
					<h4 class="text-2xl font-josefin font-light" lang="en">Visual Generation </h4>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Discrete or continuousï¼Ÿ</b> In mainstream perspectives, autoregressive models are typically associated with discrete representations. Since the essence of LLMs is to predict the next token, most vision generation models integrated with LLMs employ discrete processing approachesâ€”predicting visual tokens from a codebook followed by decoding through a pretrained VQVAE. However, visual data is inherently continuous, and the forced conversion to discrete representations followed by decoding inevitably incurs information loss[@SuJianLinBiMenZaoCheZhiDuoMoTaiSiLuQianTanYiWuSunShuRu2024]. On the other hand, modern decoder-only LLMs process continuous features via token embeddings, and their outputs before the lm head detokenization are also continuous. This suggests that visual information can inherently be handled by LLM decoders in a continuous input-output manner without requiring explicit discretization.					</p>
					</p>
				</div>
				<div class="md:col-span-5" id="imageContainer">
					<div class="rounded-lg overflow-hidden">
						<img src="diff1.png" alt="Architecture of the Diffusion model" class="w-full h-auto">
					</div>
					<br>
					<p class="text-sm text-secondary text-center italic font-josefin" lang="en">
						Architecture of the Diffusion model
					</p>
				</div>
				<div class="md:col-span-7" id="textContent">
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						In this work, for the visual generation component, we adopt a structure similar to Transfusion[@zhouTransfusionPredictNext2024], utilizing the LLM backbone for both token prediction and visual diffusion. The diffusion objective is applied to predict image patches, where noise is added to each input image <span class="math-inline"> \(x_0\) </span>  following the diffusion process, and the model predicts either the noise or the vector field. During inference, at each time step, the noise or vector field is computed, and <span class="math-inline"> \(x_{t-1}\) </span> is derived via SDE or ODE solvers, iteratively replacing <span class="math-inline">\(x_t\)</span>until  <span class="math-inline">\(x_0\)</span> is generated. In this work, our method avoids discretizing visual features and eliminates the need for an additional pretrained VAE for encoding/decoding. ã€Instead, we operate directly on raw image patches, with the diffusion process restoring the original patches. ã€‘Pretrained VAEs are often limited by their training datasets and architectures, particularly in generating high-frequency details like text, where their performance tends to be suboptimal. While LDMs (Latent Diffusion Models) offer computational efficiency advantages during both training and inference, end-to-end training from scratch enhances the modelâ€™s capability in handling such data.
					</p>
				</div>
				
				<div class="md:col-span-12">
					<h3 class="text-2xl font-josefin font-semibold mb-4" lang="en">Training Procedure</h3>
					<p class="text-lg leading-relaxed font-josefin" lang="en">
						Our framework employs distinct optimization objectives for visual understanding and generation tasks: an autoregressive objective <img src="result3.svg" alt="SVG Image" height="100" class="inline">, for visual understanding and a diffusion-based objective: <img src="result4.svg" alt="SVG Image" height="100" class="inline"> for visual generation. 					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin" lang="en">
						Owing to the decoupled design of the visual processing paths within our framework, we implement a phased training strategy. This allows the two tasks to be trained independently in the initial phase to achieve preliminary cross-modal data alignment, after which they are jointly trained. Given the substantial divergence in their optimization objectives, this segregated training approach significantly reduces the optimization difficulty compared to an end-to-end joint training regimen from the outset. Consequently, we structure the training procedure into three primary phases: 1) the Visual Understanding Training Phase, 2) the Visual Generation Training Phase, and 3) the Joint Training Phase. Each primary phase is further subdivided into specific stages to facilitate the refined optimization of different model components.					</p>
				</div>
				<div class="md:col-span-12" id="textContent">
					<h4 class="text-2xl font-josefin font-light" lang="en">Multimodel Understanding Training </h4>
					<br>
					<p class="text-lg leading-relaxed font-josefin" lang="en">
						For the visual understanding training, we implement a progressive pipeline comprising three distinct stages:
					</p>
					<br>
					<div class="rounded-lg overflow-hidden">
						<img src="stage1.png" alt="stage1" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
						Multimodel Understanding Training Stage
					</p>
					
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Stage 1: </b>This stage focuses on achieving preliminary modality alignment. We freeze the weights of the backbone networks and train only the newly introduced adapter layers, which include the components connecting the visual encoder and the Large Language Model backbone: the Adapter, the Cross-Attention Gate, and the key-value projectors within the cross-attention layers. Training is conducted using the LLaVA-Pretrain[] dataset.
					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Stage 2: </b>This stage aims to adapt the visual encoder's feature extraction capabilities to better align with the language model's representations. Here, we train the visual encoder and all additional modules while keeping the parameters of the LLM backbone frozen. Although visual encoders are typically frozen in most research to preserve generalization and training stability, some studies suggest that careful fine-tuning can enhance model performance. For this stage, we randomly sample a subset of data from the COYO[] dataset and generate a synthetic dataset with a 1:1 ratio for training.
					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Stage 3: </b>This final stage is designed to train the model's instruction-following capabilities and prepare for integration with the subsequent generative module. We train all modules except the visual encoder. Crucially, to facilitate future merging, we employ LoRI[] for the LLM backboneâ€”freezing its original weights and optimizing only the newly introduced low-rank matricesâ€”while performing standard fine-tuning on the other modules. This stage utilizes the LLaVA-Instruct[] dataset for training.
					</p>
				</div>
				<div class="md:col-span-12" id="textContent">
					<h4 class="text-2xl font-josefin font-light" lang="en">Visual Generation Training </h4>
					<br>
					<p class="text-lg leading-relaxed font-josefin" lang="en">
						The training for the visual generation component is conducted over two stages:
					</p>
					<br>
					<div class="rounded-lg overflow-hidden">
						<img src="stage2.png" alt="stage2" class="w-full h-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
						Visual Generation Training Stage
					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Stage 4: </b>In this stage, we freeze the weights of the backbone network and train only the newly initialized componentsâ€”namely, the visual generation encoder and decoder, which follow a U-Net-like architecture. The objective of this phase is to achieve preliminary feature alignment between the visual generation modules and the language model. The model is trained on the ImageNet-1K dataset, with the output image resolution set to 128Ã—128 pixels.
					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						<b>Stage 5: </b>This stage focuses on further enhancing the model's generative capability and its ability to follow instructions for image generation. Consequently, in addition to the visual generation encoder and decoder, we apply Low-Rank Adaptation to fine-tune the LLM backbone. Training is performed on a subset of N data points randomly sampled from dataset (a).
					</p>
				</div>
				<div class="md:col-span-12" id="textContent">
					<h4 class="text-2xl font-josefin font-light" lang="en">Mixed Training </h4>
					<br>
					<div class="rounded-lg overflow-hidden">
						<img src="stage3.png" alt="stage2" class="w-1/2 h-auto mx-auto">
					</div>
					<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
						Mix Training Stage
					</p>
					<br>
					<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
						Following the completion of the aforementioned five-stage training, the model has acquired foundational capabilities in both visual understanding and generation. Subsequently, joint training is conducted to integrate these two tasks. First, the incremental parameters learned via LoRA in Stage 3 and Stage 5 are concatenated and merged into the LLM backbone. In this final joint training stage, the consolidated model undergoes comprehensive fine-tuning, optimizing all parameters except those of the visual encoder. For data composition, the visual understanding component utilizes randomly sampled subsets from the datasets employed in Stage 2 and Stage 3. The visual generation component leverages a portion of the dataset from Stage 5, supplemented with an additional dataset (b). This strategy aims to enhance the model's image-to-image generation capability and its capacity for synergistic multimodal understanding.
					</p>
				</div>
			</div>
        </div>
    </section>

    <!-- ç»“æœéƒ¨åˆ† -->
    <section id="results" class="py-20 bg-white section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <span class="text-sm font-josefin uppercase tracking-widest text-accent" lang="en">Section 03</span>
            <h2 class="text-3xl md:text-4xl font-josefin font-bold mt-2 mb-12" lang="en">Results & Analysis</h2>
            <div class="md:col-span-12" id="textContent">
				<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
					Due to the constraints that the diffusion model was trained on only 400,000 synthetic data samples and for an insufficient number of steps, its performance is not yet optimal. Therefore, this study refrains from any quantitative benchmarking against other mature models, and instead presents selected generated samples alongside qualitative analysis. Furthermore, limited by the experimental setup, comprehensive ablation studies were not conducted. The comparative analyses provided are primarily preliminary explorations of observed phenomena, and readers are advised to interpret these findings with caution.
				</p>
			</div>
            <div class="md:col-span-12" id="textContent">
				<div class="rounded-lg overflow-hidden">
					<img src="output (1).png" alt="gen" class="w-full h-auto mx-auto">
				</div>
				<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
					Show some of the results generated
				</p>
			</div>
			<div class="md:col-span-12" id="textContent">
				<h3 class="text-2xl font-josefin font-semibold mb-4" lang="en">Visual Generation</h3>
				<br>
				<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
					The model training has progressed to Stage 5. Empirical results indicate that after approximately 60,000 training steps, the model achieves the generative performance illustrated in the accompanying figure. However, analysis reveals that the current model, built upon a 1.5B-parameter backbone, exhibits a relatively weak capability in generating legible text within images. We attribute this limitation primarily to the scarcity of training data explicitly containing textual information, with only about 10,000 relevant samples, constituting roughly 1/40th of the entire dataset.
				</p>
				<br>
				<div class="rounded-lg overflow-hidden">
					<img src="stage3.png" alt="stage2" class="w-1/2 h-auto mx-auto">
				</div>
				<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
					Mix Training Stage
				</p>
				<br>
				<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
					The model exhibits certain limitations in generating human figures, stemming from the training protocol. Specifically, the ImageNet dataset used in Stage 4 lacks person categories, resulting in insufficient learning of human representations. While the CelebA dataset was introduced in Stage 5 to address this gap, it led to an imbalance in the data distribution. Consequently, the model overfits the portrait-style prompts prevalent in CelebA and underperforms on generating a broader spectrum of human figures.
				</p>
				<br>
				<div class="rounded-lg overflow-hidden">
					<img src="stage3.png" alt="stage2" class="w-1/2 h-auto mx-auto">
				</div>
				<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
					Mix Training Stage
				</p>
				<br>
				<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
					Compared to visual generation models utilizing discrete representations, the continuous feature representation employed in our framework preserves richer detail. Comparing Janus with the same 1.5B backbone generates some detailed information.
				</p>
				<br>
				<div class="rounded-lg overflow-hidden">
					<img src="stage3.png" alt="stage2" class="w-1/2 h-auto mx-auto">
				</div>
				<p class="text-sm text-secondary italic font-josefin text-center" lang="en">
					Mix Training Stage
				</p>
				<br>
				<p class="text-lg leading-relaxed font-josefin balanced-text" lang="en">
					A critical architectural finding concerns the role of skip connections. We observed that without skip connections between the encoder and decoder, the generative training failed to converge effectively, regardless of architectural complexity. In contrast, adopting a U-Net-like skip connection mechanism enabled rapid loss convergence within the first epoch, even when using simple residual blocks. We hypothesize that skip connections mitigate the suppression of high-frequency features (e.g., edges and contours) by dominant low-frequency information during deep feature extraction. A deeper investigation of this phenomenon is reserved for the discussion section.
				</p>
			</div>
            
        </div>
    </section>

    <!-- è®¨è®ºéƒ¨åˆ† -->
    <section id="discussion" class="py-20 bg section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <span class="text-sm font-josefin uppercase tracking-widest text-accent" lang="en">Section 04</span>
            <h2 class="text-3xl md:text-4xl font-josefin font-bold mt-2 mb-12" lang="en">discussion</h2>

			<div class="grid grid-cols-1 lg:grid-cols-2 gap-16 items-center mb-16">
                <div>
                    <p class="text-lg leading-relaxed font-alimama mb-6" lang="zh">
						Content to be supplemented
					</p>

				</div>
                
            </div>
        </div>
    </section>

    <!-- ç»“è®ºéƒ¨åˆ† -->
    <section id="conclusion" class="py-20 bg-white section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <span class="text-sm font-josefin uppercase tracking-widest text-accent" lang="en">Section 05</span>
            <h2 class="text-3xl md:text-4xl font-josefin font-bold mt-2 mb-12" lang="en">Future Work</h2>
            
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-16 items-center mb-16">
                <div>
                    <p class="text-lg leading-relaxed font-alimama mb-6" lang="zh">
						Content to be supplemented
					</p>

				</div>
                
            </div>
        </div>
    </section>

    <!-- å‚è€ƒæ–‡çŒ® -->
    <section class="py-20 section-fade-in">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <h2 class="text-3xl font-josefin font-bold mb-10" lang="en">References</h2>
            
            <ul class="space-y-6 pl-4 border-l-2 border-gray-200">
                <li class="font-josefin" lang="en">
					Content to be supplemented
				</li>
            </ul>
        </div>
    </section>

    <!-- é¡µè„š 
    <footer class="bg-primary text-white py-16">
        <div class="container mx-auto max-w-6xl px-4 md:px-8">
            <div class="grid grid-cols-1 md:grid-cols-12 gap-12 mb-12">
                <div class="md:col-span-5">
                    <div class="flex items-center space-x-1 mb-6">
                        <span class="text-2xl font-josefin font-bold tracking-wider" lang="en">TECH REPORT</span>
                        <span class="h-4 w-px bg-white/30 mx-2"></span>
                        <span class="text-lg font-alimama" lang="zh">æŠ€æœ¯æŠ¥å‘Š</span>
                    </div>
                    <p class="text-white/70 font-alimama mb-6" lang="zh">
                        æœ¬æŠ¥å‘Šç”±æŠ€æœ¯ç ”ç©¶å›¢é˜Ÿæ’°å†™ï¼Œæ—¨åœ¨åˆ†äº«å‰æ²¿ç³»ç»Ÿæ¨¡å‹çš„ç ”ç©¶æˆæœï¼Œä¿ƒè¿›æŠ€æœ¯äº¤æµä¸åˆ›æ–°ã€‚
                    </p>
                    <div class="flex space-x-4">
                        <a href="#" class="w-10 h-10 rounded-full bg-white/10 flex items-center justify-center hover:bg-accent transition-colors">
                            <i class="fa fa-twitter"></i>
                        </a>
                        <a href="#" class="w-10 h-10 rounded-full bg-white/10 flex items-center justify-center hover:bg-accent transition-colors">
                            <i class="fa fa-linkedin"></i>
                        </a>
                        <a href="#" class="w-10 h-10 rounded-full bg-white/10 flex items-center justify-center hover:bg-accent transition-colors">
                            <i class="fa fa-github"></i>
                        </a>
                    </div>
                </div>
                
                <div class="md:col-span-3">
                    <h3 class="text-lg font-josefin font-semibold mb-6" lang="en">Sections</h3>
                    <ul class="space-y-3">
                        <li><a href="#intro" class="text-white/70 hover:text-white transition-colors font-josefin" lang="en">Introduction</a></li>
                        <li><a href="#methodology" class="text-white/70 hover:text-white transition-colors font-josefin" lang="en">Methodology</a></li>
                        <li><a href="#results" class="text-white/70 hover:text-white transition-colors font-josefin" lang="en">Results</a></li>
                        <li><a href="#discussion" class="text-white/70 hover:text-white transition-colors font-alimama" lang="zh">è®¨è®º</a></li>
                        <li><a href="#conclusion" class="text-white/70 hover:text-white transition-colors font-josefin" lang="en">Conclusion</a></li>
                    </ul>
                </div>
                
                
            </div>
            
        </div>
    </footer>
	-->

    <!-- è¿”å›é¡¶éƒ¨æŒ‰é’® -->
    <button id="backToTop" class="fixed bottom-8 right-8 w-12 h-12 bg-accent text-white rounded-full flex items-center justify-center shadow-lg opacity-0 invisible transition-all duration-300 hover:bg-accent/90">
        <i class="fa fa-arrow-up"></i>
    </button>

    <script>
        // å¯¼èˆªæ æ»šåŠ¨æ•ˆæœ
        const navbar = document.getElementById('navbar');
        const backToTop = document.getElementById('backToTop');
        
        window.addEventListener('scroll', function() {
            if (window.scrollY > 100) {
                navbar.classList.add('bg-white', 'shadow-md', 'py-4');
                navbar.classList.remove('py-6');
                
                backToTop.classList.remove('opacity-0', 'invisible');
                backToTop.classList.add('opacity-100', 'visible');
            } else {
                navbar.classList.remove('bg-white', 'shadow-md', 'py-4');
                navbar.classList.add('py-6');
                
                backToTop.classList.add('opacity-0', 'invisible');
                backToTop.classList.remove('opacity-100', 'visible');
            }
            
            // æ»šåŠ¨åŠ¨ç”»
            const sections = document.querySelectorAll('.section-fade-in');
            sections.forEach(section => {
                const sectionTop = section.getBoundingClientRect().top;
                const windowHeight = window.innerHeight;
                
                if (sectionTop < windowHeight * 0.75) {
                    section.classList.add('section-visible');
                }
            });
        });
        
        // ç§»åŠ¨ç«¯èœå•
        const menuToggle = document.getElementById('menuToggle');
        const mobileMenu = document.getElementById('mobileMenu');
        
        menuToggle.addEventListener('click', function() {
            if (mobileMenu.classList.contains('opacity-0')) {
                mobileMenu.classList.remove('opacity-0', '-translate-y-full', 'pointer-events-none');
                mobileMenu.classList.add('opacity-100', 'translate-y-0', 'pointer-events-auto');
                menuToggle.innerHTML = '<i class="fa fa-times"></i>';
            } else {
                mobileMenu.classList.add('opacity-0', '-translate-y-full', 'pointer-events-none');
                mobileMenu.classList.remove('opacity-100', 'translate-y-0', 'pointer-events-auto');
                menuToggle.innerHTML = '<i class="fa fa-bars"></i>';
            }
        });
        
        // ç§»åŠ¨ç«¯èœå•ç‚¹å‡»åå…³é—­
        const mobileLinks = mobileMenu.querySelectorAll('a');
        mobileLinks.forEach(link => {
            link.addEventListener('click', function() {
                mobileMenu.classList.add('opacity-0', '-translate-y-full', 'pointer-events-none');
                mobileMenu.classList.remove('opacity-100', 'translate-y-0', 'pointer-events-auto');
                menuToggle.innerHTML = '<i class="fa fa-bars"></i>';
            });
        });
        
        // è¿”å›é¡¶éƒ¨
        backToTop.addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });
        
        // åˆå§‹æ£€æŸ¥å¯è§åŒºåŸŸ
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section-fade-in');
            sections.forEach(section => {
                const sectionTop = section.getBoundingClientRect().top;
                const windowHeight = window.innerHeight;
                
                if (sectionTop < windowHeight * 0.75) {
                    section.classList.add('section-visible');
                }
            });
        });
    </script>
	
	<script>
		function adjustLayout() {
		  const textContent = document.getElementById('textContent');
		  const imageContainer = document.getElementById('imageContainer');
		  
		  // æ¯”è¾ƒæ–‡æœ¬å’Œå›¾ç‰‡çš„é«˜åº¦
		  if (textContent.offsetHeight > imageContainer.offsetHeight) {
			// æ–‡æœ¬æ›´é•¿æ—¶ï¼Œè®©æ–‡æœ¬å æ»¡æ•´è¡Œ
			textContent.classList.remove('md:col-span-7');
			textContent.classList.add('md:col-span-12');
			imageContainer.classList.add('hidden'); // éšè—åŸå§‹å›¾ç‰‡å®¹å™¨
			
			// åœ¨æ–‡æœ¬ä¸­æ’å…¥å›¾ç‰‡ï¼ˆéœ€è¦æå‰æ ‡è®°æ’å…¥ä½ç½®ï¼‰
			const insertPoint = document.getElementById('imageInsertPoint');
			if (insertPoint && !document.getElementById('embeddedImage')) {
			  const img = document.createElement('div');
			  img.id = 'embeddedImage';
			  img.className = 'my-8 md:col-span-5 mx-auto';
			  img.innerHTML = imageContainer.innerHTML; // å¤åˆ¶å›¾ç‰‡å†…å®¹
			  insertPoint.parentNode.insertBefore(img, insertPoint);
			}
		  } else {
			// æ¢å¤é»˜è®¤å¸ƒå±€
			textContent.classList.add('md:col-span-7');
			textContent.classList.remove('md:col-span-12');
			imageContainer.classList.remove('hidden');
			
			const embeddedImg = document.getElementById('embeddedImage');
			if (embeddedImg) embeddedImg.remove();
		  }
		}

		// åˆå§‹åŒ–æ—¶æ£€æŸ¥
		window.addEventListener('load', adjustLayout);
		// çª—å£å¤§å°å˜åŒ–æ—¶é‡æ–°æ£€æŸ¥
		window.addEventListener('resize', adjustLayout);
    </script>
	
	<script>
	MathJax = {
	  chtml: {
		font: "mathjax-tex", // æ˜ç¡®æŒ‡å®šä½¿ç”¨ MathJax TeX å­—ä½“
		mtextFontInherit: false // é˜»æ­¢æ–‡æœ¬å…ƒç´ ç»§æ‰¿å¤–éƒ¨å­—ä½“ :cite[9]
	  }
	};
	</script>
	
</body>
</html>
